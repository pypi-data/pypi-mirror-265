Metadata-Version: 2.1
Name: toxicityclassifier
Version: 1.0.1
Summary: Module for determining the level of toxicity of the text
Home-page: https://github.com/D1ffic00lt/toxicity-classification-module
License: MIT
Author: D1ffic00lt
Author-email: dm.filinov@gmail.com
Requires-Python: >=3.8,<4.0
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Project-URL: Documentation, https://github.com/D1ffic00lt/toxicity-classification-module/blob/main/README.md
Project-URL: Repository, https://github.com/D1ffic00lt/toxicity-classification-module
Description-Content-Type: text/markdown

# ToxicityClassificator
Module for predicting toxicity messages in Russian and English
## Usage example
```python
from toxicityclassifier import *

classifier = ToxicityClassificatorV1()

print(classifier.predict(text))          # (0 or 1, probability)
print(classifier.get_probability(text))  # probability
print(classifier.classify(text))         # 0 or 1
```

## Weights
Weight for classification (if probability >= weight => 1 else 0)
```python
classifier.weight = 0.5
```
\
Weight for language detection (English or Russian)

if the percentage of the Russian language >= language_weight, then the Russian model is used, otherwise the English one
```python
classifier.language_weight = 0.5
```

