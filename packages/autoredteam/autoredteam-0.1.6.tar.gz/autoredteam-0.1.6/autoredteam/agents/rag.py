import os
from langchain.prompts import PromptTemplate
from langchain_community.llms.octoai_endpoint import OctoAIEndpoint
from langchain_openai import OpenAIEmbeddings
# from langchain_community.embeddings import OctoAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document

from milvus import default_server
from langchain_community.vectorstores import Milvus


# class RagAgent:
#     def __init__(
#         self,
#         llm,
#         path,
#         # embeddings=OctoAIEmbeddings(
#         #     endpoint_url="https://text.octoai.run/v1/embeddings",
#         # ),
#         embeddings=OpenAIEmbeddings(
#             model="text-embedding-3-small", dimensions=1024
#             # model="text-embedding-ada-002"
#         ),
#         generations=1,
#     ):
#         """
#         Initialize the `RagAgent` object with a name and number of generations.
        
#         Parameters
#         ----------
#         llm : str
#             name of the LLM to use.
#         path : str
#             path to the documents to index.
#         embeddings : Langchain Embeddings class
#             embeddings to use for the vector store. Defaults to OctoAIEmbeddings.
#         generations : int
#             number of generations to run.
#         """
#         # initialize the LLM and embeddings
#         print("Creating LLM Agent: " + llm)
#         self.llm = OctoAIEndpoint(
#             endpoint_url="https://text.octoai.run/v1/chat/completions",
#             model_kwargs={
#                 "model": llm,
#                 "max_tokens": 500,
#                 "presence_penalty": 0,
#                 "temperature": 0.01,
#                 "top_p": 0.9,
#                 "messages": [
#                     {
#                         "role": "system",
#                         "content": "You are a helpful assistant. Keep your responses limited to one short paragraph if possible.",
#                     },
#                 ],
#             },
#         )
#         self.embeddings = embeddings
#         self.generations = generations

#         # load and chunk the documents
#         print("Loading documents from: " + path)
#         self.path = path
#         files = [f for f in os.listdir(self.path) if f.endswith(".txt")]
#         file_texts = []
#         for file in files:
#             with open(f"{self.path}/{file}") as f:
#                 file_text = f.read()
#             text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
#                 chunk_size=1024,
#                 chunk_overlap=64,
#             )
#             texts = text_splitter.split_text(file_text)
#             for i, chunked_text in enumerate(texts):
#                 file_texts.append(
#                     Document(
#                         page_content=chunked_text,
#                         metadata={"doc_title": file.split(".")[0], "chunk_num": i},
#                     )
#                 )

#         # create the vector store
#         print("Creating vector db server")
#         default_server.start()

#         print("Creating vector store")
#         self.vector_store = Milvus(
#             embedding_function=self.embeddings    
#         ).from_documents(
#             file_texts,
#             embedding=self.embeddings,
#             connection_args={"host": "localhost", "port": default_server.listen_port},
#             collection_name="cities",
#         )
#         self.retriever = self.vector_store.as_retriever()

#         # set up the chain
#         template = """Answer the question based only on the following context:
#         {context}


#         Question: {question}
#         """
#         prompt = PromptTemplate.from_template(template)
#         from langchain_core.runnables import RunnablePassthrough
#         from langchain_core.output_parsers import StrOutputParser

#         self.chain = (
#             {"context": self.retriever, "question": RunnablePassthrough()}
#             | prompt
#             | self.llm
#             | StrOutputParser()
#         )

#     def generate(self, prompt):
#         """
#         Generate response(s) for a given prompt.
        
#         Parameters
#         ----------
#         prompt : str
#             prompt to generate response for.
            
#         Returns
#         -------
#         list[str]
#             response generated by the model.
#         """
#         return [self.chain.invoke(prompt) for _ in range(self.generations)]

from embedchain import App


class RagAgent:
    """Use Embedchain to instantiate a RAG agent."""
    def __init__(self, config: str, files: str, generations: int = 1):
        """
        Initialize the `RagAgent` object with a name and number of generations.
        
        Parameters
        ----------
        path : str
            path to the documents to index.
        generations : int
            number of generations to run.
        """
        # load and chunk the documents
        self.config_path = config
        self.agent = App().from_config(config_path=self.config_path)
        for file in files:
            print(f"Adding {file}")
            self.agent.add(file, data_type='pdf_file')
        self.generations = generations
            
    def generate(self, prompt):
        """Call the model with a given prompt."""
        return [
            self.agent.query(prompt)
            for _ in range(self.generations)
        ]
