Metadata-Version: 2.1
Name: llama-index-llms-ipex-llm
Version: 0.1.0
Summary: llama-index llms ipex-llm integration
License: MIT
Author: Your Name
Author-email: you@example.com
Requires-Python: >=3.9,<4.0
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Dist: ipex-llm[all]
Requires-Dist: llama-index-core (>=0.10.0,<0.11.0)
Requires-Dist: torch (<2.2.0)
Description-Content-Type: text/markdown

# LlamaIndex Llms Integration: IPEX-LLM

[IPEX-LLM](https://github.com/intel-analytics/ipex-llm) is a PyTorch library for running LLM on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max) with very low latency. This module allows loading LLMs with ipex-llm optimizations.

