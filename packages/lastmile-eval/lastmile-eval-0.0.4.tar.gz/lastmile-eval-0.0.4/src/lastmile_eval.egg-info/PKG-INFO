Metadata-Version: 2.1
Name: lastmile-eval
Version: 0.0.4
Summary: An API to measure evaluation criteria (ex: faithfulness) of generative AI outputs
Author: LastMile AI
Project-URL: Homepage, https://github.com/lastmile-ai/eval
Project-URL: Bug Tracker, https://github.com/lastmile-ai/eval/issues
Classifier: Intended Audience :: Developers
Classifier: Programming Language :: Python :: 3
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: dataclasses
Requires-Dist: requests
Requires-Dist: python-dotenv
Requires-Dist: rouge_score
Requires-Dist: nltk
Requires-Dist: openai>=1.0.0
Requires-Dist: evaluate==0.4.1
Requires-Dist: arize-phoenix-evals==0.5.0
Requires-Dist: pandas==2.1.2
Requires-Dist: instructor

# LastMile AI Eval

Library of tools to evaluate your RAG system.

## Setup

1. Get a LastMile API token (see section below)
2. Install this library: `pip install lastmile-eval`
3. Gather your data that needs evaluation.
4. See examples/ folder for API usage.

## LastMile API token

To get a LastMile AI token, please go to the [LastMile token's webpage](https://lastmileai.dev/settings?page=tokens).
You can create an account with Google or Github and then click the "Create new token" in the "API Tokens" section. Once a token is created, be sure to save it somewhere since you won't be able to see the value of it from the website again (though you can create a new one if that happens).

**Please be careful not to share your token on GitHub**. Instead we recommend saving it under your projectâ€™s (or home directory) `.env` file as: `LASTMILE_API_TOKEN=<TOKEN_HERE>`, and use loadenv instead.
See examples/ for how to do this.


## LLM Provider Tokens (`.env` file)

In order to use LLM-based evaluators, add your other API tokens to your .env file.
Example: `OPENAI_API_KEY=<TOKEN_HERE>`
